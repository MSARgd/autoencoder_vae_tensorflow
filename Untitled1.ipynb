{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d22c7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_vae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_vae\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sigmoid, lrelu, tanh, img_tile, mnist_reader, relu, BCE_loss\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m():\n\u001b[1;32m      7\u001b[0m     parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_vae'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as npy\n",
    "import os\n",
    "from utils_vae import sigmoid, lrelu, tanh, img_tile, mnist_reader, relu, BCE_loss\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epoch\", type=int, default=40)\n",
    "    parser.add_argument(\"--nz\", type=int, default=20)\n",
    "    parser.add_argument(\"--layersize\", type=int, default=400)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--b1\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--b2\", type=float, default=0.999)\n",
    "    parser.add_argument(\"--e\", type=float, default=1e-8)\n",
    "    parser.add_argument(\"--bsize\", type=int, default=64)\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "cpu_enabled = 0\n",
    "try:\n",
    "    import cupy as np\n",
    "    cpu_enabled = 1\n",
    "except ImportError:\n",
    "    import numpy as np\n",
    "    print(\"CuPy not enabled on this machine\")\n",
    "    \n",
    "\n",
    "np.random.seed(111)\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self, numbers):\n",
    "        \n",
    "        self.numbers = numbers\n",
    "\n",
    "        self.epochs = args.epoch\n",
    "        self.batch_size = args.bsize\n",
    "        self.learning_rate = args.lr\n",
    "        self.decay = 0.001\n",
    "        self.nz = args.nz\n",
    "        self.layersize = args.layersize\n",
    "\n",
    "        self.img_path = \"./images\"\n",
    "        if not os.path.exists(self.img_path):\n",
    "                os.makedirs(self.img_path)\n",
    "        \n",
    "        # Xavier initialization is used to initialize the weights\n",
    "        # init encoder weights\n",
    "        self.e_W0 = np.random.randn(784, self.layersize).astype(np.float32) * np.sqrt(2.0/(784))\n",
    "        self.e_b0 = np.zeros(self.layersize).astype(np.float32)\n",
    "\n",
    "        self.e_W_mu = np.random.randn(self.layersize, self.nz).astype(np.float32) * np.sqrt(2.0/(self.layersize))\n",
    "        self.e_b_mu = np.zeros(self.nz).astype(np.float32)\n",
    "        \n",
    "        self.e_W_logvar = np.random.randn(self.layersize, self.nz).astype(np.float32) * np.sqrt(2.0/(self.layersize))\n",
    "        self.e_b_logvar = np.zeros(self.nz).astype(np.float32)\n",
    "\n",
    "        # init decoder weights \n",
    "        self.d_W0 = np.random.randn(self.nz, self.layersize).astype(np.float32) * np.sqrt(2.0/(self.nz))\n",
    "        self.d_b0 = np.zeros(self.layersize).astype(np.float32)\n",
    "        \n",
    "        self.d_W1 = np.random.randn(self.layersize, 784).astype(np.float32) * np.sqrt(2.0/(self.layersize))\n",
    "        self.d_b1 = np.zeros(784).astype(np.float32)\n",
    "             \n",
    "        # init sample\n",
    "        self.sample_z = 0\n",
    "        self.rand_sample = 0\n",
    "        \n",
    "        # init Adam optimizer\n",
    "        self.b1 = args.b1\n",
    "        self.b2 = args.b2\n",
    "        self.e = args.e\n",
    "        self.m = [0] * 10\n",
    "        self.v = [0] * 10\n",
    "        self.t = 0\n",
    "        \n",
    "    def encoder(self, img):\n",
    "        #self.e_logvar : log variance \n",
    "        #self.e_mean : mean\n",
    "\n",
    "        self.e_input = np.reshape(img, (self.batch_size,-1))\n",
    "    \n",
    "        self.e_h0_l = self.e_input.dot(self.e_W0) + self.e_b0\n",
    "        self.e_h0_a = lrelu(self.e_h0_l)\n",
    "    \t\t\n",
    "        self.e_logvar = self.e_h0_a.dot(self.e_W_logvar) + self.e_b_logvar\n",
    "        self.e_mu = self.e_h0_a.dot(self.e_W_mu) + self.e_b_mu\n",
    "    \n",
    "        return self.e_mu, self.e_logvar\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        #self.d_out : reconstruction image 28x28\n",
    "\t\t\n",
    "        self.z = np.reshape(z, (self.batch_size, self.nz))\n",
    "        \n",
    "        self.d_h0_l = self.z.dot(self.d_W0) + self.d_b0\t\t\n",
    "        self.d_h0_a = relu(self.d_h0_l)\n",
    "\n",
    "        self.d_h1_l = self.d_h0_a.dot(self.d_W1) + self.d_b1\n",
    "        self.d_h1_a = sigmoid(self.d_h1_l)\n",
    "\n",
    "        self.d_out = np.reshape(self.d_h1_a, (self.batch_size, 28, 28, 1))\n",
    "\n",
    "        return self.d_out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Encode\n",
    "        mu, logvar = self.encoder(x)\n",
    "        \n",
    "        #use reparameterization trick to sample from gaussian\n",
    "        self.rand_sample = np.random.standard_normal(size=(self.batch_size, self.nz))\n",
    "        self.sample_z = mu + np.exp(logvar * .5) * np.random.standard_normal(size=(self.batch_size, self.nz))\n",
    "        \n",
    "        decode = self.decoder(self.sample_z)\n",
    "        \n",
    "        return decode, mu, logvar\n",
    "    \n",
    "    def backward(self, x, out):\n",
    "        ########################################\n",
    "        #Calculate gradients from reconstruction\n",
    "        ########################################\n",
    "        y = np.reshape(x, (self.batch_size, -1))\n",
    "        out = np.reshape(out, (self.batch_size, -1))\n",
    "        \n",
    "        #Calculate decoder gradients\n",
    "        #Left side term\n",
    "        dL_l = -y * (1 / out)\n",
    "        dsig = sigmoid(self.d_h1_l, derivative=True)\n",
    "        dL_dsig_l = dL_l * dsig\n",
    "        \n",
    "        drelu = relu(self.d_h0_l, derivative=True)\n",
    "\n",
    "        dW1_d_l = np.matmul(np.expand_dims(self.d_h0_a, axis=-1), np.expand_dims(dL_dsig_l, axis=1))\n",
    "        db1_d_l = dL_dsig_l \n",
    "        \n",
    "        db0_d_l = dL_dsig_l.dot(self.d_W1.T) * drelu\n",
    "        dW0_d_l = np.matmul(np.expand_dims(self.sample_z, axis=-1), np.expand_dims(db0_d_l, axis=1))\n",
    "        \n",
    "        #Right side term\n",
    "        dL_r = (1 - y) * (1 / (1 - out))\n",
    "        dL_dsig_r = dL_r * dsig\n",
    "        \n",
    "        dW1_d_r = np.matmul(np.expand_dims(self.d_h0_a, axis=-1), np.expand_dims(dL_dsig_r, axis=1))\n",
    "        db1_d_r = dL_dsig_r\n",
    "        \n",
    "        db0_d_r = dL_dsig_r.dot(self.d_W1.T) * drelu\n",
    "        dW0_d_r = np.matmul(np.expand_dims(self.sample_z, axis=-1), np.expand_dims(db0_d_r, axis=1))\n",
    "        \n",
    "        # Combine gradients for decoder\n",
    "        grad_d_W0 = dW0_d_l + dW0_d_r\n",
    "        grad_d_b0 = db0_d_l + db0_d_r\n",
    "        grad_d_W1 = dW1_d_l + dW1_d_r\n",
    "        grad_d_b1 = db1_d_l + db1_d_r\n",
    "         \n",
    "        #Calculate encoder gradients from reconstruction\n",
    "        #Left side term\n",
    "        d_b_mu_l  = db0_d_l.dot(self.d_W0.T)\n",
    "        d_W_mu_l = np.matmul(np.expand_dims(self.e_h0_a, axis=-1), np.expand_dims(d_b_mu_l, axis=1))\n",
    "        \n",
    "        db0_e_l = d_b_mu_l.dot(self.e_W_mu.T) * lrelu(self.e_h0_l, derivative=True)\n",
    "        dW0_e_l = np.matmul(np.expand_dims(y, axis=-1), np.expand_dims(db0_e_l, axis=1)) \n",
    "        \n",
    "        d_b_logvar_l = d_b_mu_l * np.exp(self.e_logvar * .5) * .5 * self.rand_sample\n",
    "        d_W_logvar_l = np.matmul(np.expand_dims(self.e_h0_a, axis=-1), np.expand_dims(d_b_logvar_l, axis=1))\n",
    "        \n",
    "        db0_e_l_2 = d_b_logvar_l.dot(self.e_W_logvar.T) * lrelu(self.e_h0_l, derivative=True)\n",
    "        dW0_e_l_2 = np.matmul(np.expand_dims(y, axis=-1), np.expand_dims(db0_e_l_2, axis=1)) \n",
    "        \n",
    "        #Right side term\n",
    "        d_b_mu_r  = db0_d_r.dot(self.d_W0.T)\n",
    "        d_W_mu_r = np.matmul(np.expand_dims(self.e_h0_a, axis=-1), np.expand_dims(d_b_mu_r, axis=1))\n",
    "        \n",
    "        db0_e_r = d_b_mu_r.dot(self.e_W_mu.T) * lrelu(self.e_h0_l, derivative=True)\n",
    "        dW0_e_r = np.matmul(np.expand_dims(y, axis=-1), np.expand_dims(db0_e_r, axis=1)) \n",
    "        \n",
    "        d_b_logvar_r = d_b_mu_r * np.exp(self.e_logvar * .5) * .5 * self.rand_sample\n",
    "        d_W_logvar_r = np.matmul(np.expand_dims(self.e_h0_a, axis=-1), np.expand_dims(d_b_logvar_r, axis=1))\n",
    "        \n",
    "        db0_e_r_2 = d_b_logvar_r.dot(self.e_W_logvar.T) * lrelu(self.e_h0_l, derivative=True)\n",
    "        dW0_e_r_2 = np.matmul(np.expand_dims(y, axis=-1), np.expand_dims(db0_e_r_2, axis=1))\n",
    "        \n",
    "        ########################################\n",
    "        #Calculate encoder gradients from K-L\n",
    "        ########################################\n",
    "    \n",
    "        #logvar terms\n",
    "        dKL_b_log = -.5 * (1 - np.exp(self.e_logvar))\n",
    "        dKL_W_log = np.matmul(np.expand_dims(self.e_h0_a, axis= -1), np.expand_dims(dKL_b_log, axis= 1))\n",
    "        \n",
    "        #Heaviside step function\n",
    "        dlrelu = lrelu(self.e_h0_l, derivative=True)  \n",
    "\n",
    "        dKL_e_b0_1 = .5 * dlrelu * (np.exp(self.e_logvar) - 1).dot(self.e_W_logvar.T)\n",
    "        dKL_e_W0_1 = np.matmul(np.expand_dims(y, axis= -1), np.expand_dims(dKL_e_b0_1, axis= 1))\n",
    "        \n",
    "        #m^2 term\n",
    "        dKL_W_m = .5 * (2 * np.matmul(np.expand_dims(self.e_h0_a, axis=-1), np.expand_dims(self.e_mu, axis=1)))\n",
    "        dKL_b_m = .5 * (2 * self.e_mu)\n",
    "        \n",
    "        dKL_e_b0_2 = .5 * dlrelu * (2 * self.e_mu).dot(self.e_W_mu.T)\n",
    "        dKL_e_W0_2 = np.matmul(np.expand_dims(y, axis= -1), np.expand_dims(dKL_e_b0_2, axis= 1))\n",
    "        \n",
    "        # Combine gradients for encoder from recon and KL\n",
    "        grad_b_logvar = dKL_b_log + d_b_logvar_l + d_b_logvar_r\n",
    "        grad_W_logvar = dKL_W_log + d_W_logvar_l + d_W_logvar_r\n",
    "        grad_b_mu = dKL_b_m + d_b_mu_l + d_b_mu_r\n",
    "        grad_W_mu = dKL_W_m + d_W_mu_l + d_W_mu_r\n",
    "        grad_e_b0 = dKL_e_b0_1 + dKL_e_b0_2 + db0_e_l + db0_e_l_2 + db0_e_r + db0_e_r_2\n",
    "        grad_e_W0 = dKL_e_W0_1 + dKL_e_W0_2 + dW0_e_l + dW0_e_l_2 + dW0_e_r + dW0_e_r_2\n",
    "        \n",
    "        \n",
    "        grad_list = [grad_e_W0, grad_e_b0, grad_W_mu, grad_b_mu, grad_W_logvar, grad_b_logvar,\n",
    "                     grad_d_W0, grad_d_b0, grad_d_W1, grad_d_b1]\n",
    "        \n",
    "        ########################################\n",
    "        #Calculate update using Adam\n",
    "        ########################################\n",
    "        self.t += 1\n",
    "        for i, grad in enumerate(grad_list):\n",
    "            self.m[i] = self.b1 * self.m[i] + (1 - self.b1) * grad\n",
    "            self.v[i] = self.b2 * self.v[i] + (1 - self.b2) * np.power(grad, 2)\n",
    "            m_h = self.m[i] / (1 - (self.b1 ** self.t))\n",
    "            v_h = self.v[i] / (1 - (self.b2 ** self.t))\n",
    "            grad_list[i] = m_h / (np.sqrt(v_h) + self.e)\n",
    "        \n",
    "        # Update all weights\n",
    "        for idx in range(self.batch_size):\n",
    "            # Encoder Weights\n",
    "            self.e_W0 = self.e_W0 - self.learning_rate*grad_list[0][idx]\n",
    "            self.e_b0 = self.e_b0 - self.learning_rate*grad_list[1][idx]\n",
    "    \n",
    "            self.e_W_mu = self.e_W_mu - self.learning_rate*grad_list[2][idx]\n",
    "            self.e_b_mu = self.e_b_mu - self.learning_rate*grad_list[3][idx]\n",
    "            \n",
    "            self.e_W_logvar = self.e_W_logvar - self.learning_rate*grad_list[4][idx]\n",
    "            self.e_b_logvar = self.e_b_logvar - self.learning_rate*grad_list[5][idx]\n",
    "    \n",
    "            # Decoder Weights\n",
    "            self.d_W0 = self.d_W0 - self.learning_rate*grad_list[6][idx]\n",
    "            self.d_b0 = self.d_b0 - self.learning_rate*grad_list[7][idx]\n",
    "            \n",
    "            self.d_W1 = self.d_W1 - self.learning_rate*grad_list[8][idx]\n",
    "            self.d_b1 = self.d_b1 - self.learning_rate*grad_list[9][idx]\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        #Read in training data\n",
    "        trainX, _, train_size = mnist_reader(self.numbers)\n",
    "        \n",
    "        np.random.shuffle(trainX)\n",
    "        \n",
    "        #set batch indices\n",
    "        batch_idx = train_size//self.batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_kl = 0\n",
    "        total = 0\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for idx in range(batch_idx):\n",
    "                # prepare batch and input vector z\n",
    "                train_batch = trainX[idx*self.batch_size:idx*self.batch_size + self.batch_size]\n",
    "                #ignore batch if there are insufficient elements \n",
    "                if train_batch.shape[0] != self.batch_size:\n",
    "                    break\n",
    "                \n",
    "                ################################\n",
    "                #\t\tForward Pass\n",
    "                ################################\n",
    "                \n",
    "                out, mu, logvar = self.forward(train_batch)\n",
    "                \n",
    "                # Reconstruction Loss\n",
    "                rec_loss = BCE_loss(out, train_batch)\n",
    "                \n",
    "                #K-L Divergence\n",
    "                kl = -0.5 * np.sum(1 + logvar - np.power(mu, 2) - np.exp(logvar))\n",
    "                \n",
    "                loss = rec_loss + kl\n",
    "                loss = loss / self.batch_size\n",
    "                \n",
    "                #Loss Recordkeeping\n",
    "                total_loss += rec_loss / self.batch_size\n",
    "                total_kl += kl / self.batch_size\n",
    "                total += 1\n",
    "\n",
    "                ################################\n",
    "                #\t\tBackward Pass\n",
    "                ################################\n",
    "                # for every result in the batch\n",
    "                # calculate gradient and update the weights using Adam\n",
    "                self.backward(train_batch, out)\t\n",
    "\n",
    "                self.img = np.squeeze(out, axis=3) * 2 - 1\n",
    "\n",
    "                print(\"Epoch [%d] Step [%d]  RC Loss:%.4f  KL Loss:%.4f  lr: %.4f\"%(\n",
    "                        epoch, idx, rec_loss / self.batch_size, kl / self.batch_size, self.learning_rate))\n",
    "                \n",
    "            if cpu_enabled == 1:\n",
    "                sample = np.array(self.img)\n",
    "            else: \n",
    "                sample = np.asnumpy(self.img)\n",
    "            \n",
    "            #save image result every epoch\n",
    "            img_tile(sample, self.img_path, epoch, idx, \"res\", True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Adjust the numbers that appear in the training data. Less numbers helps \n",
    "    # run the program to see faster results\n",
    "    numbers = [1, 2, 3]\n",
    "    model = VAE(numbers)\n",
    "    model.train()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
