{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "77ae2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Loss: 7.117923877115204, Reconstruction Loss: 0.2693485322042783, KL Loss: 6.848575344910926\n",
      "Epoch 2, Total Loss: 5.432230799063259, Reconstruction Loss: 0.9833913398334598, KL Loss: 4.448839459229799\n",
      "Epoch 4, Total Loss: 0.35868101489420723, Reconstruction Loss: 0.22273170306412515, KL Loss: 0.13594931183008208\n",
      "Epoch 6, Total Loss: 0.9513764997356096, Reconstruction Loss: 0.21455757775457843, KL Loss: 0.7368189219810312\n",
      "Epoch 8, Total Loss: 0.7229217805817156, Reconstruction Loss: 0.6696698170895313, KL Loss: 0.05325196349218436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_dim = 10  # Dimension of the input data\n",
    "latent_dim = 2  # Dimension of the latent space\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Xavier initialization for encoder and decoder weights\n",
    "encoder_weights = np.random.randn(input_dim, latent_dim) * np.sqrt(2 / (input_dim + latent_dim))\n",
    "encoder_biases = np.zeros(latent_dim)\n",
    "decoder_weights = np.random.randn(latent_dim, input_dim) * np.sqrt(2 / (latent_dim + input_dim))\n",
    "decoder_biases = np.zeros(input_dim)\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the reparameterization trick\n",
    "def reparameterize(mu, log_var):\n",
    "    epsilon = np.random.normal(0, 1, mu.shape)\n",
    "    return mu + np.exp(log_var / 2) * epsilon\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Generate random input data\n",
    "    input_data = np.random.randn(input_dim)\n",
    "\n",
    "    # Encoder forward pass\n",
    "    encoder_output = np.dot(input_data, encoder_weights) + encoder_biases\n",
    "    encoder_mu = encoder_output\n",
    "    encoder_log_var = encoder_output  # For simplicity, assume log_var is the same as the output\n",
    "\n",
    "    # Reparameterization\n",
    "    z = reparameterize(encoder_mu, encoder_log_var)\n",
    "\n",
    "    # Decoder forward pass\n",
    "    decoder_output = np.dot(z, decoder_weights) + decoder_biases\n",
    "    reconstructed_data = sigmoid(decoder_output)\n",
    "\n",
    "    # Calculate the loss\n",
    "    reconstruction_loss = np.mean((input_data - reconstructed_data) ** 2)\n",
    "    kl_loss = -0.5 * np.sum(1 + encoder_log_var - encoder_mu**2 - np.exp(encoder_log_var))\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "    # Compute gradients\n",
    "    d_reconstruction_loss = 2 * (reconstructed_data - input_data)\n",
    "    d_decoder_weights = np.outer(z, d_reconstruction_loss)\n",
    "    d_decoder_biases = d_reconstruction_loss\n",
    "\n",
    "    d_kl_loss = -0.5 * (1 + encoder_log_var - encoder_mu**2 - np.exp(encoder_log_var))\n",
    "    d_encoder_weights = np.outer(input_data, d_kl_loss)\n",
    "    d_encoder_biases = d_kl_loss\n",
    "\n",
    "    # Backpropagation\n",
    "    decoder_weights -= learning_rate * d_decoder_weights\n",
    "    decoder_biases -= learning_rate * d_decoder_biases\n",
    "    encoder_weights -= learning_rate * d_encoder_weights\n",
    "    encoder_biases -= learning_rate * d_encoder_biases\n",
    "\n",
    "    # Print the loss\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}, Total Loss: {total_loss}, Reconstruction Loss: {reconstruction_loss}, KL Loss: {kl_loss}\")\n",
    "\n",
    "# After training, you can use the encoder and decoder for encoding and decoding data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183c32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8b74d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Data:\n",
      "[-2.53778453  0.28785156  0.85917366  0.36815674  0.19538945 -1.00184319\n",
      " -0.42567049 -0.50451291 -0.78232566 -1.17210336]\n",
      "Generated Data:\n",
      "[0.32841906 0.88337369 0.82459167 0.31339585 0.75555472 0.12151455\n",
      " 0.56934713 0.5133678  0.47649427 0.39565512]\n"
     ]
    }
   ],
   "source": [
    "# Function to encode data using the learned encoder weights and biases\n",
    "def encode_data(data):\n",
    "    encoder_output = np.dot(data, encoder_weights) + encoder_biases\n",
    "    return encoder_output\n",
    "\n",
    "# Function to decode encoded data using the learned decoder weights and biases\n",
    "def decode_data(encoded_data):\n",
    "    decoder_output = np.dot(encoded_data, decoder_weights) + decoder_biases\n",
    "    return sigmoid(decoder_output)\n",
    "\n",
    "# Generate random input data for testing\n",
    "test_data = np.random.randn(input_dim)\n",
    "\n",
    "# Encode the test data\n",
    "encoded_test_data = encode_data(test_data)\n",
    "\n",
    "# Decode the encoded data to generate new data\n",
    "generated_data = decode_data(encoded_test_data)\n",
    "\n",
    "# Print the original test data and the generated data\n",
    "print(\"Original Test Data:\")\n",
    "print(test_data)\n",
    "print(\"Generated Data:\")\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea626098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
